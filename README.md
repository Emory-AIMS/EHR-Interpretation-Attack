# Interpretation Attacks on Interpretable Models with Electronic Health Records
<!-- ![Python 3.6](https://img.shields.io/badge/python-3.6-DodgerBlue.svg?style=plastic)
![Tensorflow 1.13.1](https://img.shields.io/badge/Tensorflow-1.13.1-DodgerBlue.svg?style=plastic)
![Keras 2.2.5](https://img.shields.io/badge/Keras-2.2.5-DodgerBlue.svg?style=plastic)
 -->

This paper presents the first interpretation attack on electronic health records (EHRs). Our research shows that our attack can attain significant success on EHR interpretations that do not rely on model gradients. We introduce metrics compatible with EHR data to evaluate the attack's success. Moreover, our findings demonstrate that detection methods that have successfully identified conventional adversarial examples are ineffective against our attack. This paper also proposes a defense method utilizing auto-encoders to de-noise the data and improve the interpretations' robustness.


<!--## Citation

```bibtex
@article{razmi2021classification,
  title={Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks},
  author={Razmi, Fereshteh and Xiong, Li},
  journal={arXiv preprint arXiv:2108.04206},
  year={2021}
}
```

## How to run

The code consists of four sequential parts; generating indices, conducting the poisoining attacks, training the detectors and finally utilizing the detectors for the defense. The following commands are meant to replicate the results of the experiments in the paper for CIFAR-10 dataset.


#### Generating indices

First indices of training/test/validation data for cross validation on all the runs (60 in the paper) are generated by:

```
python attack.py —attack_step index_generation —dataset cifar10 
``` 

#### Conducting the poisoining attacks

These commands generate poisoned points for each attack indicated in the paper:

```
python attack.py —attack_step poison_generation —dataset cifar10 --attack_type flipping 
python attack.py —attack_step poison_generation —dataset cifar10 --attack_type optimal 
python attack.py —attack_step poison_generation —dataset cifar10 --attack_type opt-notlabel 
python attack.py —attack_step poison_generation —dataset cifar10 --attack_type mixed
``` 

#### Training the detectors

Next step is to train detectors CAE+ (including RAE and CAE) and Magnets (including two models). Training is done on different percentages of poisoned data 
and different attack types. 
All can be done by the shell script below. This codes takes a while to be completed. It is better to run this section on a GPU machine.

```
 ./run_train.sh
```

#### Evaluating the detectors

Finally detectors performance can be assessed using this command:

```
python assess_detectors.py —dataset cifar10 
```

## Acknowledgement

We modified the code from [SecML repository](https://github.com/pralab/secml) to generate the optimal attacks. The modified version is included in the current repository. 
 -->